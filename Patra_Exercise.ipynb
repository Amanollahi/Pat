{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBgsRDDW/INC34pCt01aJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanollahi/Pat/blob/main/Patra_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOhBk0WZYcAQ",
        "outputId": "b926c744-e666-4334-ab14-c1c5b70e0d03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = \"https://raw.githubusercontent.com/Amanollahi/Pat/main/review_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Check the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Check the shape of your dataset\n",
        "print(\"\\nDataset shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFLxXcvKaGi6",
        "outputId": "8b49fc33-785f-46aa-de99-a0a0f1ee8814"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review  label\n",
            "0  After 6 years of faithful service, my hard dri...      0\n",
            "1  So far so good - this is a comprehensive and p...      0\n",
            "2  This has been a great and easy software to use...      0\n",
            "3  This router is great. The setup and installati...      0\n",
            "4  Overview\\n\\nThis is a great array for someone ...      0\n",
            "\n",
            "Dataset shape: (3825, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "I8hDUUYSa7jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess the text data.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    \"\"\"Apply preprocessing to the dataset.\"\"\"\n",
        "    df['cleaned_text'] = df['review'].apply(preprocess_text)  # Fixed column name\n",
        "    return df\n",
        "\n",
        "def vectorize_text(train_texts, test_texts):\n",
        "    \"\"\"Convert text data into numerical format using TF-IDF.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "    return X_train, X_test, vectorizer\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def random_search_hyperparameters(model, param_dist, X_train, y_train, n_iter=50):\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_dist,\n",
        "        scoring='f1_weighted',\n",
        "        n_iter=n_iter,\n",
        "        cv=5,\n",
        "        verbose=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(\"Best F1 Score:\", random_search.best_score_)\n",
        "    return random_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "def train_Logis_model(X_train, y_train):\n",
        "    \"\"\"Train a Logistic Regression model.\"\"\"\n",
        "    # Added class_weight='balanced' to handle class imbalance\n",
        "    model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate the model using F1 score and other metrics.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(\"\\nF1 Score:\", f1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return f1\n",
        "\n",
        "def main():\n",
        "    file_path = \"https://raw.githubusercontent.com/Amanollahi/Pat/main/review_data.csv\"\n",
        "\n",
        "    # Load and preprocess\n",
        "    df = load_data(file_path)\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Split dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['cleaned_text'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Feature engineering and training\n",
        "    X_train_vec, X_test_vec, vectorizer = vectorize_text(X_train, X_test)\n",
        "    model = train_Logis_model(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZhDVk_H0Pc5",
        "outputId": "602ece42-a582-4b33-8cbf-4cac1ae54d70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3825 entries, 0 to 3824\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3824 non-null   object\n",
            " 1   label   3825 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 59.9+ KB\n",
            "None\n",
            "\n",
            "Class Distribution:\n",
            "label\n",
            "0    3738\n",
            "1      87\n",
            "Name: count, dtype: int64\n",
            "\n",
            "F1 Score: 0.9600424696076871\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       748\n",
            "           1       0.11      0.12      0.11        17\n",
            "\n",
            "    accuracy                           0.96       765\n",
            "   macro avg       0.55      0.55      0.55       765\n",
            "weighted avg       0.96      0.96      0.96       765\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "kVFYqxdlbcD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess the text data.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    \"\"\"Apply preprocessing to the dataset.\"\"\"\n",
        "    df['cleaned_text'] = df['review'].apply(preprocess_text)  # Fixed column name\n",
        "    return df\n",
        "\n",
        "def vectorize_text(train_texts, test_texts):\n",
        "    \"\"\"Convert text data into numerical format using TF-IDF.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "    return X_train, X_test, vectorizer\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    model = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate the model using F1 score and other metrics.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(\"\\nF1 Score:\", f1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return f1\n",
        "\n",
        "def main():\n",
        "    file_path = \"https://raw.githubusercontent.com/Amanollahi/Pat/main/review_data.csv\"\n",
        "\n",
        "    # Load and preprocess\n",
        "    df = load_data(file_path)\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Split dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['cleaned_text'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Feature engineering and training\n",
        "    X_train_vec, X_test_vec, vectorizer = vectorize_text(X_train, X_test)\n",
        "    model = train_model(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr561gyNbBm6",
        "outputId": "221c2394-718d-4407-b35e-635861808ebe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3825 entries, 0 to 3824\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3824 non-null   object\n",
            " 1   label   3825 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 59.9+ KB\n",
            "None\n",
            "\n",
            "Class Distribution:\n",
            "label\n",
            "0    3738\n",
            "1      87\n",
            "Name: count, dtype: int64\n",
            "\n",
            "F1 Score: 0.9617220703439087\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       748\n",
            "           1       0.08      0.06      0.07        17\n",
            "\n",
            "    accuracy                           0.96       765\n",
            "   macro avg       0.53      0.52      0.53       765\n",
            "weighted avg       0.96      0.96      0.96       765\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "0xUQzcrxb9lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess the text data.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    \"\"\"Apply preprocessing to the dataset.\"\"\"\n",
        "    df['cleaned_text'] = df['review'].apply(preprocess_text)  # Fixed column name\n",
        "    return df\n",
        "\n",
        "def vectorize_text(train_texts, test_texts):\n",
        "    \"\"\"Convert text data into numerical format using TF-IDF.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "    return X_train, X_test, vectorizer\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate the model using F1 score and other metrics.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(\"\\nF1 Score:\", f1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return f1\n",
        "\n",
        "def main():\n",
        "    file_path = \"https://raw.githubusercontent.com/Amanollahi/Pat/main/review_data.csv\"\n",
        "\n",
        "    # Load and preprocess\n",
        "    df = load_data(file_path)\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Split dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['cleaned_text'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Feature engineering and training\n",
        "    X_train_vec, X_test_vec, vectorizer = vectorize_text(X_train, X_test)\n",
        "    model = train_model(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOaT8tcnb7I6",
        "outputId": "c2121cad-04f9-4ca7-fcad-8fc4c5cb3d35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3825 entries, 0 to 3824\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3824 non-null   object\n",
            " 1   label   3825 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 59.9+ KB\n",
            "None\n",
            "\n",
            "Class Distribution:\n",
            "label\n",
            "0    3738\n",
            "1      87\n",
            "Name: count, dtype: int64\n",
            "\n",
            "F1 Score: 0.9608747044917257\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       748\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.97       765\n",
            "   macro avg       0.49      0.49      0.49       765\n",
            "weighted avg       0.96      0.97      0.96       765\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBOOST"
      ],
      "metadata": {
        "id": "nRA078LPcki7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XV-1wwReGNr",
        "outputId": "bb1daffe-515b-4ba2-8c23-a9baf5401fc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess the text data.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    \"\"\"Apply preprocessing to the dataset.\"\"\"\n",
        "    df['cleaned_text'] = df['review'].apply(preprocess_text)  # Fixed column name\n",
        "    return df\n",
        "\n",
        "def vectorize_text(train_texts, test_texts):\n",
        "    \"\"\"Convert text data into numerical format using TF-IDF.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "    return X_train, X_test, vectorizer\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def train_xgboost(X_train, y_train):\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=10, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def train_lightgbm(X_train, y_train):\n",
        "    model = LGBMClassifier(class_weight='balanced', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "def train_catboost(X_train, y_train):\n",
        "    model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "def train_naive_bayes(X_train, y_train):\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate the model using F1 score and other metrics.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(\"\\nF1 Score:\", f1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return f1\n",
        "\n",
        "def main():\n",
        "    file_path = \"https://raw.githubusercontent.com/Amanollahi/Pat/main/review_data.csv\"\n",
        "\n",
        "    # Load and preprocess\n",
        "    df = load_data(file_path)\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Split dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['cleaned_text'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Feature engineering and training\n",
        "    X_train_vec, X_test_vec, vectorizer = vectorize_text(X_train, X_test)\n",
        "    model = train_xgboost(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "\n",
        "    model = train_lightgbm(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "    model = train_catboost(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "    model = train_naive_bayes(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlTOs4R8cWdQ",
        "outputId": "e31d238d-3ea6-4cfe-d460-4b896540be50"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3825 entries, 0 to 3824\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3824 non-null   object\n",
            " 1   label   3825 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 59.9+ KB\n",
            "None\n",
            "\n",
            "Class Distribution:\n",
            "label\n",
            "0    3738\n",
            "1      87\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:15:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "F1 Score: 0.9595502293238645\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       748\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.96       765\n",
            "   macro avg       0.49      0.49      0.49       765\n",
            "weighted avg       0.96      0.96      0.96       765\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 70, number of negative: 2990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048650 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 38401\n",
            "[LightGBM] [Info] Number of data points in the train set: 3060, number of used features: 1428\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "F1 Score: 0.9608747044917257\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       748\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.97       765\n",
            "   macro avg       0.49      0.49      0.49       765\n",
            "weighted avg       0.96      0.97      0.96       765\n",
            "\n",
            "\n",
            "F1 Score: 0.9654827560850062\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       748\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.98       765\n",
            "   macro avg       0.49      0.50      0.49       765\n",
            "weighted avg       0.96      0.98      0.97       765\n",
            "\n",
            "\n",
            "F1 Score: 0.9667915106117354\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       748\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.98       765\n",
            "   macro avg       0.49      0.50      0.49       765\n",
            "weighted avg       0.96      0.98      0.97       765\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ensamble models"
      ],
      "metadata": {
        "id": "XhZj4J2iffIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess the text data.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_dataset(df):\n",
        "    \"\"\"Apply preprocessing to the dataset.\"\"\"\n",
        "    df['cleaned_text'] = df['review'].apply(preprocess_text)  # Fixed column name\n",
        "    return df\n",
        "\n",
        "def vectorize_text(train_texts, test_texts):\n",
        "    \"\"\"Convert text data into numerical format using TF-IDF.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "    return X_train, X_test, vectorizer\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "def train_ensemble(X_train, y_train):\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    model1 = LogisticRegression(class_weight='balanced', random_state=42)\n",
        "    model2 = SVC(kernel='linear', class_weight='balanced', random_state=42, probability=True)\n",
        "    model3 = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "\n",
        "    ensemble = VotingClassifier(estimators=[\n",
        "        ('lr', model1), ('svc', model2), ('rf', model3)\n",
        "    ], voting='soft')\n",
        "\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    return ensemble\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate the model using F1 score and other metrics.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(\"\\nF1 Score:\", f1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return f1\n",
        "\n",
        "def main():\n",
        "    file_path = \"https://raw.githubusercontent.com/Amanollahi/Pat/main/review_data.csv\"\n",
        "\n",
        "    # Load and preprocess\n",
        "    df = load_data(file_path)\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Split dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['cleaned_text'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Feature engineering and training\n",
        "    X_train_vec, X_test_vec, vectorizer = vectorize_text(X_train, X_test)\n",
        "    model = train_ensemble(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_model(model, X_test_vec, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHxoHNwmfd_G",
        "outputId": "679385cf-eadb-4443-a513-4f1155f3d76f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3825 entries, 0 to 3824\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3824 non-null   object\n",
            " 1   label   3825 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 59.9+ KB\n",
            "None\n",
            "\n",
            "Class Distribution:\n",
            "label\n",
            "0    3738\n",
            "1      87\n",
            "Name: count, dtype: int64\n",
            "\n",
            "F1 Score: 0.9608747044917257\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       748\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.97       765\n",
            "   macro avg       0.49      0.49      0.49       765\n",
            "weighted avg       0.96      0.97      0.96       765\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUMzyikegC27"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}